{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#debezium-bigquery-consumers","title":"Debezium Bigquery Consumers","text":"<p>This project adds Bigquery consumers to Debezium Server. These consumers replicate Given Database to Bigquery in real time.</p> <ul> <li>Debezium Bigquery Consumers<ul> <li><code>bigquerybatch</code> Consumer (Uses BQ Free API)</li> <li><code>bigquerystream</code> Consumer</li> </ul> </li> </ul>"},{"location":"#install-from-source","title":"Install from source","text":"<ul> <li>Requirements:<ul> <li>JDK 21</li> <li>Maven</li> </ul> </li> <li>Clone from repo: <code>git clone https://github.com/memiiso/debezium-server-bigquery.git</code></li> <li>From the root of the project:<ul> <li>Build and package debezium server: <code>mvn -Passembly -Dmaven.test.skip package</code></li> <li>After building, unzip your server   distribution: <code>unzip debezium-server-bigquery-dist/target/debezium-server-bigquery-dist*.zip -d appdist</code></li> <li>cd into unzipped folder: <code>cd appdist</code></li> <li>Create <code>application.properties</code> file and config it: <code>nano conf/application.properties</code>, you can check the example   configuration   in application.properties.example</li> <li>Run the server using provided script: <code>bash run.sh</code></li> </ul> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>The Memiiso community welcomes anyone that wants to help out in any way, whether that includes reporting problems, helping with documentation, or contributing code changes to fix bugs, add tests, or implement new features.</p>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"bigquerybatch/","title":"<code>bigquerybatch</code> Consumer","text":"<p>Writes debezium events to Bigquery using BigQuery Storage Write API. It groups CDC events and appends to destination BigQuery table using BigQuery Write API</p> <p>Note</p> <p>This consumer only supports append mode.</p> <p>Info</p> <p>This consumer uses free api to import data to bigquery</p>"},{"location":"bigquerybatch/#configuration","title":"Configuration","text":"Config Default Description <code>debezium.sink.bigquerybatch.dataset</code> Destination Bigquery dataset name <code>debezium.sink.bigquerybatch.location</code> <code>US</code> Bigquery table location <code>debezium.sink.bigquerybatch.project</code> Bigquery project <code>debezium.sink.bigquerybatch.create-disposition</code> <code>CREATE_IF_NEEDED</code> Create tables if needed <code>debezium.sink.bigquerybatch.partition-field</code> <code>__ts_ms</code> Partition target tables by <code>__ts_ms</code> field <code>debezium.sink.bigquerybatch.clustering-field</code> <code>__source_ts_ms</code> Cluster target tables by <code>PK + __source_ts_ms</code> field <code>debezium.sink.bigquerybatch.partition-type</code> <code>MONTH</code> Partitioning type <code>debezium.sink.bigquerybatch.allow-field-addition</code> <code>true</code> Allow field addition to target tables <code>debezium.sink.bigquerybatch.allow-field-relaxation</code> <code>true</code> Allow field relaxation <code>debezium.sink.bigquerybatch.credentials-file</code> GCP service account credentialsFile <code>debezium.sink.bigquerybatch.cast-deleted-field</code> <code>false</code> Cast deleted field to boolean type(by default it is string type) <code>debezium.sink.bigquerybatch.writeDisposition</code> <code>WRITE_APPEND</code> Specifies the action that occurs if the destination table or partition already exists. <code>debezium.sink.bigquerybatch.bigquery-custom-host</code> Custom endpoint for BigQuery API. Useful for testing against a local BigQuery emulator like <code>bq-emulator</code>. <code>debezium.sink.bigquerybatch.bigquery-dev-emulator</code> <code>false</code> Whether or not Debezium should connect to <code>bq-emulator</code> instance."},{"location":"bigquerystream/","title":"<code>bigquerystream</code> Consumer","text":"<p>Streams debezium events to Bigquery using the Storage Write API.</p>"},{"location":"bigquerystream/#configuration","title":"Configuration","text":"Config Default Description <code>debezium.sink.bigquerystream.dataset</code> Destination Bigquery dataset name <code>debezium.sink.bigquerystream.location</code> <code>US</code> Bigquery table location <code>debezium.sink.bigquerystream.project</code> Bigquery project <code>debezium.sink.bigquerystream.ignore-unknown-fields</code> <code>true</code> if true, unknown Json fields to BigQuery will be ignored instead of error out. <code>debezium.sink.bigquerystream.create-if-needed</code> <code>true</code> Creates Bigquery table if not found <code>debezium.sink.bigquerystream.partition-field</code> <code>__ts_ms</code> Partition target tables by <code>__ts_ms</code> field <code>debezium.sink.bigquerystream.clustering-field</code> <code>__source_ts_ms</code> Cluster target tables by <code>PK + __source_ts_ms</code> field <code>debezium.sink.bigquerystream.partition-type</code> <code>MONTH</code> Partitioning type <code>debezium.sink.bigquerystream.allow-field-addition</code> <code>false</code> Allow field addition to target tables <code>debezium.sink.bigquerystream.credentials-file</code> GCP service account credentialsFile <code>debezium.sink.bigquerystream.bigquery-custom-host</code> Custom endpoint for BigQuery API. Useful for testing against a local BigQuery emulator like <code>bq-emulator</code>. <code>debezium.sink.bigquerystream.bigquery-custom-grpc-host</code> Custom endpoint for BigQuery GRPC API. Useful for testing against a local BigQuery emulator like <code>bq-emulator</code>. <code>debezium.sink.bigquerystream.bigquery-dev-emulator</code> <code>false</code> Whether or not Debezium should connect to <code>bq-emulator</code> instance. <code>debezium.sink.bigquerystream.upsert</code> <code>false</code> Running upsert mode overwriting updated rows. Using Bigquery CDC feature <code>debezium.sink.bigquerystream.upsert-keep-deletes</code> <code>true</code> With upsert mode, keeps deleted rows in bigquery table. <code>debezium.sink.bigquerystream.upsert-dedup-column</code> <code>__source_ts_ns</code> With upsert mode used to deduplicate data. row with highest <code>__source_ts_ns</code> is kept. <code>debezium.sink.bigquerystream.upsert-op-column</code> <code>__op</code> Used with upsert mode to deduplicate data when <code>__source_ts_ns</code> of rows are same. <code>debezium.sink.bigquerystream.cast-deleted-field</code> <code>false</code> Cast deleted field to boolean type(by default it is string type)"},{"location":"bigquerystream/#upsert","title":"Upsert","text":"<p>By default, Bigquery Streaming consumer is running with append mode <code>debezium.sink.bigquerystream.upsert=false</code>. Upsert mode uses source Primary Key and does upsert on target table(delete followed by insert). For the tables without Primary Key consumer falls back to append mode.</p>"},{"location":"bigquerystream/#upsert-mode-data-deduplication","title":"Upsert Mode Data Deduplication","text":"<p>With upsert mode data deduplication is done. Deduplication is done based on <code>__source_ts_ns</code> value and event type <code>__op</code> . its is possible to change this field using <code>debezium.sink.bigquerystream.upsert-dedup-column=__source_ts_ns</code> (Currently only Long field type supported.)</p> <p>Operation type priorities are <code>{\"c\":1, \"r\":2, \"u\":3, \"d\":4}</code>. When two records with same key and same <code>__source_ts_ns</code> values received then the record with higher <code>__op</code> priority is kept and added to destination table and duplicate record is dropped from stream.</p>"},{"location":"configuration/","title":"Shared Configs for both consumers","text":"Config Default Description <code>debezium.sink.batch.destination-regexp</code> `` Regexp to modify destination. With this its possible to map <code>table_ptt1</code>,<code>table_ptt2</code> to <code>table_combined</code>. <code>debezium.sink.batch.destination-regexp-replace</code> `` Regexp Replace part to modify destination <code>debezium.sink.batch.batch-size-wait</code> <code>NoBatchSizeWait</code> Batch size wait strategy to optimize data files and upload interval. explained below. <code>debezium.sink.batch.batch-size-wait.max-wait-ms</code> <code>300000</code> <code>debezium.sink.batch.batch-size-wait.wait-interval-ms</code> <code>10000</code> <code>debezium.source.max.batch.size</code> <code>2048</code> <code>debezium.format.value</code> <code>json</code> <code>debezium.format.key</code> <code>json</code> <code>debezium.source.time.precision.mode</code> <code>isostring</code> <code>debezium.source.decimal.handling.mode</code> <code>double</code> <code>debezium.format.value.schemas.enable</code> <code>true</code> <code>debezium.format.key.schemas.enable</code> <code>true</code> <code>debezium.source.offset.storage</code> <code>io.debezium.server.bigquery.offset.BigqueryOffsetBackingStore</code> <code>debezium.source.offset.storage.bigquery.table-name</code> <code>_debezium_offset_storage</code> <code>debezium.source.schema.history.internal</code> <code>io.debezium.server.bigquery.history.BigquerySchemaHistory</code> <code>debezium.source.schema.history.internal.bigquery.table-name</code> <code>_debezium_database_history_storage</code> <code>debezium.transforms</code> <code>unwrap</code> <code>debezium.transforms.unwrap.type</code> <code>io.debezium.transforms.ExtractNewRecordState</code> <code>debezium.transforms.unwrap.add.fields</code> <code>op,table,source.ts_ms,db,ts_ms,ts_ns,source.ts_ns</code> <code>debezium.transforms.unwrap.delete.tombstone.handling.mode</code> <code>rewrite</code> <code>debezium.transforms.unwrap.drop.tombstones</code> <code>true</code>"},{"location":"configuration/#data-type-mapping","title":"Data type mapping","text":"<p>Data type mapping listed below.</p> Debezium Semantic Type Debezium Field Type Bigquery Batch Bigquery Stream Notes int8-int64 INT64 INT64 io.debezium.time.Date int32 DATE DATE io.debezium.time.Timestamp int64 INT64 INT64 io.debezium.time.MicroTimestamp int64 INT64 INT64 io.debezium.time.NanoTimestamp int64 INT64 INT64 io.debezium.time.IsoDate string DATE DATE io.debezium.time.IsoTimestamp string DATETIME DATETIME io.debezium.time.IsoTime string TIME TIME io.debezium.time.ZonedTimestamp string TIMESTAMP TIMESTAMP io.debezium.time.ZonedTime string TIME (STRING before&lt;0.8.0.Beta) TIME (STRING before &lt;0.8.0.Beta) io.debezium.data.Json string JSON JSON io.debezium.data.geometry.Geometry struct STRUCT(srid:INT64, wkb:GEOGRAPHY) STRUCT(srid:INT64, wkb:GEOGRAPHY) version <code>0.8.0.Beta</code> and above. string STRING STRING double FLOAT64 FLOAT64 float8-float64 FLOAT64 FLOAT64 boolean BOOL BOOL bytes BYTES BYTES (STRING before &lt;0.8.0.Beta) array ARRAY ARRAY map STRUCT STRUCT struct STRUCT STRUCT <p>Handling of special fields:</p> Field Name Debezium Semantic Type Debezium Field Type Bigquery Batch Bigquery Stream Notes <code>__ts_ms</code>, <code>__source_ts_ms</code> int64 TIMESTAMP TIMESTAMP <code>__deleted</code> string BOOL BOOL"},{"location":"configuration/#mandatory-config","title":"Mandatory config","text":""},{"location":"configuration/#debezium-event-format-and-schema","title":"Debezium Event format and schema","text":"<pre><code>debezium.format.value=json\ndebezium.format.key=json\ndebezium.format.schemas.enable=true\n</code></pre>"},{"location":"configuration/#flattening-event-data","title":"Flattening Event Data","text":"<p>Bigquery consumers requires event flattening, please see debezium feature</p> <pre><code>debezium.transforms=unwrap\ndebezium.transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\ndebezium.transforms.unwrap.add.fields=op,table,lsn,source.ts_ms,source.ts_ns\ndebezium.transforms.unwrap.add.headers=db\ndebezium.transforms.unwrap.delete.tombstone.handling.mode=rewrite\ndebezium.transforms.unwrap.drop.tombstones=true\n</code></pre>"},{"location":"configuration/#optimizing-batch-size-or-commit-interval","title":"Optimizing batch size (or commit interval)","text":"<p>Debezium extracts database events in real time and this could cause too frequent commits or too many small files which is not optimal for batch processing especially when near realtime data feed is sufficient. To avoid this problem following batch-size-wait classes are used.</p> <p>Batch size wait adds delay between consumer calls to increase total number of events received per call and meanwhile events are collected in memory. This setting should be configured together with <code>debezium.source.max.queue.size</code> and <code>debezium.source.max.batch.size</code> debezium properties</p>"},{"location":"configuration/#nobatchsizewait","title":"NoBatchSizeWait","text":"<p>This is default configuration, by default consumer will not use any wait. All the events are consumed immediately.</p>"},{"location":"configuration/#maxbatchsizewait","title":"MaxBatchSizeWait","text":"<p>MaxBatchSizeWait uses debezium metrics to optimize batch size. MaxBatchSizeWait periodically reads streaming queue current size and waits until number of events reaches to <code>max.batch.size</code> or until <code>debezium.sink.batch.batch-size-wait.max-wait-ms</code>.</p> <p>Maximum wait and check intervals are controlled by <code>debezium.sink.batch.batch-size-wait.max-wait-ms</code> , <code>debezium.sink.batch.batch-size-wait.wait-interval-ms</code> properties.</p> <p>example setup to receive ~2048 events per commit. maximum wait is set to 30 seconds, streaming queue current size checked every 5 seconds</p> <pre><code>debezium.sink.batch.batch-size-wait=MaxBatchSizeWait\ndebezium.sink.batch.metrics.snapshot-mbean=debezium.postgres:type=connector-metrics,context=snapshot,server=testc\ndebezium.sink.batch.metrics.streaming-mbean=debezium.postgres:type=connector-metrics,context=streaming,server=testc\ndebezium.source.connector.class=io.debezium.connector.postgresql.PostgresConnector\ndebezium.source.max.batch.size=2048;\ndebezium.source.max.queue.size=16000\";\ndebezium.sink.batch.batch-size-wait.max-wait-ms=30000\ndebezium.sink.batch.batch-size-wait.wait-interval-ms=5000\n</code></pre>"},{"location":"configuration/#bigquery-offset-storage","title":"Bigquery Offset Storage","text":"<p>This implementation saves CDC offset to a bigquery table, along the destination data. With this no additional dependency required to manage the application.</p> <pre><code>debezium.source.offset.storage=io.debezium.server.bigquery.offset.BigqueryOffsetBackingStore\ndebezium.source.offset.storage.bigquery.table-name=debezium_offset_storage_custom_table\n</code></pre>"},{"location":"configuration/#bigquery-database-history-storage","title":"Bigquery Database History Storage","text":"<p>This implementation saves database history to a bigquery table, along the destination data. With this no additional dependency required to manage the application.</p> <pre><code>debezium.source.database.history=io.debezium.server.bigquery.history.BigquerySchemaHistory\ndebezium.source.database.history.bigquery.table-name=__debezium_database_history_storage_test_table\n</code></pre>"},{"location":"configuration/#configuring-log-levels","title":"Configuring log levels","text":"<pre><code>quarkus.log.level=INFO\n# Ignore messages below warning level from Jetty, because it's a bit verbose\nquarkus.log.category.\"org.eclipse.jetty\".level=WARN\n#\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Debezium BigQuey consumer is a very young project and looking for new maintainers. There are definitively many small/big improvements to do, including documentation, adding new features to submitting bug reports.</p> <p>Please feel free to send pull request, report bugs or open feature request.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under Apache 2.0 License.</p>"}]}